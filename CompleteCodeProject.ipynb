{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CompleteCodeProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VVX82a0r3My9",
        "uv58DTpS33pc",
        "YpiOjgIw4JaG",
        "DWaxJn4wTlkx",
        "heTZbdUw4eHo"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saquibirtiza/Senti-Stance-Detector/blob/main/CompleteCodeProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwqiskg43lFs"
      },
      "source": [
        "# **Convert JSONL to CSV and preprocess the text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVX82a0r3My9"
      },
      "source": [
        "## Clean tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRGeukztzVRc",
        "outputId": "7cbe8346-8b49-4061-a818-55215cfd7a0e"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import json\n",
        "import csv\n",
        "import io\n",
        "import re\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# remove any links from the tweets\n",
        "def remove_links(tweet):\n",
        "    '''Takes a string and removes web links from it'''\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)  # remove http links\n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
        "    tweet = tweet.strip('[link]')  # remove [links]\n",
        "    return tweet\n",
        "\n",
        "# remove the usenames from the tweets\n",
        "def remove_users(tweet):\n",
        "    '''Takes a string and removes retweet and @user information'''\n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)',\n",
        "                   '', tweet)  # remove retweet\n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)',\n",
        "                   '', tweet)  # remove tweeted at\n",
        "    return tweet\n",
        "\n",
        "\n",
        "my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "\n",
        "\n",
        "# cleaning master function\n",
        "def clean_tweet(tweet, bigrams=False):\n",
        "    tweet = remove_users(tweet)\n",
        "    tweet = remove_links(tweet)\n",
        "    tweet = tweet.lower()  # lower case\n",
        "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet)  # strip punctuation\n",
        "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
        "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
        "    tweet_token_list = [word for word in tweet.split(' ')\n",
        "                        if word not in my_stopwords]  # remove stopwords\n",
        "\n",
        "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in tweet_token_list]  # apply word rooter\n",
        "    if bigrams:\n",
        "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
        "                                             for i in range(len(tweet_token_list)-1)]\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet\n",
        "\n",
        "\n",
        "# Opening JSON file and loading the data\n",
        "# into the variable data\n",
        "\n",
        "def extract_json(fileobj):\n",
        "    # Using \"with\" ensures that fileobj is closed when we finish reading it.\n",
        "    with fileobj:\n",
        "        for line in fileobj:\n",
        "            yield json.loads(line)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yZNgM1z4aaL"
      },
      "source": [
        "## Preprocess the tweet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pja5IHAk1RX4"
      },
      "source": [
        "data_json = io.open('test_single.jsonl', mode='r', encoding='utf-8')\n",
        "text_json = io.open('test_tweets.jsonl', mode='r', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9TQ4njjzaY3"
      },
      "source": [
        "# Opens in the JSONL file\n",
        "data_python = extract_json(data_json)\n",
        "text_python = extract_json(text_json)\n",
        "\n",
        "dic = {}\n",
        "count = 0\n",
        "for line in text_python:\n",
        "    id = line.get('id')\n",
        "    text = line.get('text')\n",
        "    text = clean_tweet(text)\n",
        "    dic[id] = text\n",
        "\n",
        "\n",
        "# now we will open a file for writing\n",
        "csv_out = io.open('test.csv', mode='w', encoding='utf-8')\n",
        "\n",
        "# fields = u'tweet_id,m_id,m_label'  # field names\n",
        "fields = u'tweet_id,m_id'  # field names\n",
        "csv_out.write(fields)\n",
        "csv_out.write(u'\\n')\n",
        "\n",
        "for line in data_python:\n",
        "    # writes a row and gets the fields from the json object\n",
        "    # screen_name and followers/friends are found on the second level hence two get methods\n",
        "    row = [dic[line.get('tweet_id')],\n",
        "           line.get('m_id')]\n",
        "          #  line.get('m_label')]\n",
        "    row_joined = u','.join(row)\n",
        "    csv_out.write(row_joined)\n",
        "    csv_out.write(u'\\n')\n",
        "\n",
        "csv_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv58DTpS33pc"
      },
      "source": [
        "## Combine multiple CSV files into one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDl__2gL3DY1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "all_filenames = [\"X1.csv\", \"X2.csv\", \"X3.csv\", \"X4.csv\", \"X5.csv\", \"Xtra.csv\"]\n",
        "\n",
        "# combine all files in the list\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
        "# export to csv\n",
        "combined_csv.to_csv(\"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ZDiXbB4dUt"
      },
      "source": [
        "# **Generate Sentiment labels for the test data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV79DXvO3TjI"
      },
      "source": [
        "## Import data and train sentiment analysis model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ctTicWYBtsn",
        "outputId": "3015aeab-6c50-4d8d-9ed7-7cb4d492eeb6"
      },
      "source": [
        "import torch\n",
        "\n",
        "out1 = torch.load('data1.pt')\n",
        "out2 = torch.load('data2.pt')\n",
        "out3 = torch.load('data3.pt')\n",
        "out4 = torch.load('data4.pt')\n",
        "out5 = torch.load('data5.pt')\n",
        "out6 = torch.load('data6.pt')\n",
        "out7 = torch.load('data7.pt')\n",
        "out8 = torch.load('data8.pt')\n",
        "\n",
        "out = out1 + out2 + out3 + out4 + out5 + out6 +out7 + out8\n",
        "len(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4410"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3B9EuDaky3",
        "outputId": "1546eeb6-ad0e-4831-e3fa-d4f7d804ca95"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def remove_links(tweet):\n",
        "    '''Takes a string and removes web links from it'''\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)  # remove http links\n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # rempve bitly links\n",
        "    tweet = tweet.strip('[link]')  # remove [links]\n",
        "    return tweet\n",
        "\n",
        "\n",
        "def remove_users(tweet):\n",
        "    '''Takes a string and removes retweet and @user information'''\n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)',\n",
        "                   '', tweet)  # remove retweet\n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)',\n",
        "                   '', tweet)  # remove tweeted at\n",
        "    return tweet\n",
        "\n",
        "\n",
        "my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "\n",
        "# cleaning master function\n",
        "\n",
        "\n",
        "def clean_tweet(tweet, bigrams=False):\n",
        "    tweet = remove_users(tweet)\n",
        "    tweet = remove_links(tweet)\n",
        "    tweet = tweet.lower()  # lower case\n",
        "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet)  # strip punctuation\n",
        "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
        "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
        "    tweet_token_list = [word for word in tweet.split(' ')\n",
        "                        if word not in my_stopwords]  # remove stopwords\n",
        "\n",
        "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in tweet_token_list]  # apply word rooter\n",
        "    if bigrams:\n",
        "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
        "                                             for i in range(len(tweet_token_list)-1)]\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet_token_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4ULPF66YYlo"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "TEXT = data.Field(include_lengths = True)\n",
        "LABEL = data.LabelField()\n",
        "fields = [('text', TEXT), (\"label\", LABEL)]\n",
        "temp = []\n",
        "\n",
        "\n",
        "for val in out:\n",
        "  text = clean_tweet(\" \".join(val.text))\n",
        "  temp.append(data.Example.fromlist([text, val.label] , fields)) \n",
        "\n",
        "ex_train = data.Dataset(temp, fields=fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FNlTF56QqNG"
      },
      "source": [
        "import random\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "extract_train, extract_test = ex_train.split(split_ratio=0.7, random_state=random.seed(SEED))\n",
        "extract_test, extract_valid = extract_test.split(split_ratio=0.5, random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3teEcDfPy10",
        "outputId": "9edb2ddc-c422-400f-e11e-114344bc58d2"
      },
      "source": [
        "print(len(extract_train), len(extract_valid), len(extract_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3087 661 662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoAfhRPLuWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57c0714-f082-41f1-9e62-2720744ab365"
      },
      "source": [
        "TEXT.build_vocab(extract_train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(extract_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 92027/93956 [00:03<00:00, 25035.33it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzlpJr3eLuWc"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (extract_train, extract_valid, extract_test), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text), \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n31qBsFLuWk"
      },
      "source": [
        "import torch.nn as nn\n",
        " \n",
        "class LSTM_net(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths): \n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pad_sequence(embedded, batch_first=True)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        output = self.fc1(hidden)\n",
        "        output = self.dropout(self.fc2(output))\n",
        "            \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHd97ghgm8EG"
      },
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding\n",
        "\n",
        "modelsenti = LSTM_net(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC0EJAwLLuWu"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "modelsenti.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "modelsenti.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVADtymyLuW3"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(modelsenti.parameters(), lr=learning_rate)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "modelsenti = modelsenti.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-c9zfwLuW7"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y.float()).float() #convert into float for division \n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggAOZpM6LuW_"
      },
      "source": [
        "# training function \n",
        "def train_model(model, iterator):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        text, text_lengths = batch.text\n",
        "        # print(text_lengths)\n",
        "        text_lengths = torch.as_tensor(text_lengths, dtype=torch.int64, device='cpu')\n",
        "        # print(text_lengths)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label.type_as(predictions))\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXd9-JLsLuXE"
      },
      "source": [
        "def evaluate(model, iterator):\n",
        "    \n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            text_lengths = torch.as_tensor(text_lengths, dtype=torch.int64, device='cpu')\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            \n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpk3xpKOefBx",
        "outputId": "d346222d-a032-4384-fb84-91c1243b2730"
      },
      "source": [
        "import time \n",
        "\n",
        "t = time.time()\n",
        "loss=[]\n",
        "acc=[]\n",
        "val_acc=[]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_model(modelsenti, train_iterator)\n",
        "    valid_acc = evaluate(modelsenti, valid_iterator)\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
        "    \n",
        "    loss.append(train_loss)\n",
        "    acc.append(train_acc)\n",
        "    val_acc.append(valid_acc)\n",
        "    \n",
        "print(f'time:{time.time()-t:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.599 | Train Acc: 31.58%\n",
            "\t Val. Acc: 28.03%\n",
            "\tTrain Loss: 0.473 | Train Acc: 37.60%\n",
            "\t Val. Acc: 40.40%\n",
            "\tTrain Loss: -8.070 | Train Acc: 46.03%\n",
            "\t Val. Acc: 30.02%\n",
            "\tTrain Loss: -80.919 | Train Acc: 48.12%\n",
            "\t Val. Acc: 46.25%\n",
            "\tTrain Loss: -272.896 | Train Acc: 48.90%\n",
            "\t Val. Acc: 30.59%\n",
            "\tTrain Loss: -375.353 | Train Acc: 47.33%\n",
            "\t Val. Acc: 39.39%\n",
            "\tTrain Loss: 713.897 | Train Acc: 31.85%\n",
            "\t Val. Acc: 44.07%\n",
            "\tTrain Loss: -326.840 | Train Acc: 39.81%\n",
            "\t Val. Acc: 43.22%\n",
            "\tTrain Loss: -539.185 | Train Acc: 40.30%\n",
            "\t Val. Acc: 47.54%\n",
            "\tTrain Loss: -1053.384 | Train Acc: 45.16%\n",
            "\t Val. Acc: 46.84%\n",
            "\tTrain Loss: -795.019 | Train Acc: 40.51%\n",
            "\t Val. Acc: 49.05%\n",
            "\tTrain Loss: -805.049 | Train Acc: 41.71%\n",
            "\t Val. Acc: 47.50%\n",
            "\tTrain Loss: -1637.591 | Train Acc: 41.58%\n",
            "\t Val. Acc: 35.47%\n",
            "\tTrain Loss: -3054.111 | Train Acc: 42.27%\n",
            "\t Val. Acc: 48.34%\n",
            "\tTrain Loss: -3416.099 | Train Acc: 43.62%\n",
            "\t Val. Acc: 43.83%\n",
            "\tTrain Loss: -3684.687 | Train Acc: 44.10%\n",
            "\t Val. Acc: 51.61%\n",
            "\tTrain Loss: -5514.760 | Train Acc: 45.76%\n",
            "\t Val. Acc: 53.78%\n",
            "\tTrain Loss: -6948.531 | Train Acc: 46.28%\n",
            "\t Val. Acc: 48.90%\n",
            "\tTrain Loss: -6596.327 | Train Acc: 45.24%\n",
            "\t Val. Acc: 48.48%\n",
            "\tTrain Loss: -12013.895 | Train Acc: 49.37%\n",
            "\t Val. Acc: 43.13%\n",
            "\tTrain Loss: -15042.346 | Train Acc: 49.93%\n",
            "\t Val. Acc: 49.40%\n",
            "\tTrain Loss: -14229.129 | Train Acc: 48.50%\n",
            "\t Val. Acc: 44.14%\n",
            "\tTrain Loss: -16609.961 | Train Acc: 47.91%\n",
            "\t Val. Acc: 47.53%\n",
            "\tTrain Loss: -16278.393 | Train Acc: 49.01%\n",
            "\t Val. Acc: 46.82%\n",
            "\tTrain Loss: -21285.687 | Train Acc: 49.69%\n",
            "\t Val. Acc: 48.68%\n",
            "\tTrain Loss: -25066.992 | Train Acc: 49.52%\n",
            "\t Val. Acc: 30.45%\n",
            "\tTrain Loss: -30121.865 | Train Acc: 47.87%\n",
            "\t Val. Acc: 38.68%\n",
            "\tTrain Loss: -35559.986 | Train Acc: 47.27%\n",
            "\t Val. Acc: 50.24%\n",
            "\tTrain Loss: -39054.472 | Train Acc: 51.26%\n",
            "\t Val. Acc: 45.98%\n",
            "\tTrain Loss: -48638.130 | Train Acc: 48.30%\n",
            "\t Val. Acc: 42.83%\n",
            "\tTrain Loss: -56021.054 | Train Acc: 53.22%\n",
            "\t Val. Acc: 49.39%\n",
            "\tTrain Loss: -65978.897 | Train Acc: 54.35%\n",
            "\t Val. Acc: 51.95%\n",
            "\tTrain Loss: -68750.542 | Train Acc: 51.51%\n",
            "\t Val. Acc: 45.42%\n",
            "\tTrain Loss: -75895.252 | Train Acc: 53.10%\n",
            "\t Val. Acc: 39.74%\n",
            "\tTrain Loss: -46619.852 | Train Acc: 47.05%\n",
            "\t Val. Acc: 29.73%\n",
            "\tTrain Loss: 6439.441 | Train Acc: 38.28%\n",
            "\t Val. Acc: 42.41%\n",
            "\tTrain Loss: -76593.495 | Train Acc: 57.03%\n",
            "\t Val. Acc: 48.83%\n",
            "\tTrain Loss: -99913.299 | Train Acc: 60.34%\n",
            "\t Val. Acc: 54.52%\n",
            "\tTrain Loss: -105607.623 | Train Acc: 58.59%\n",
            "\t Val. Acc: 43.28%\n",
            "\tTrain Loss: -115368.775 | Train Acc: 61.36%\n",
            "\t Val. Acc: 54.65%\n",
            "\tTrain Loss: -135652.107 | Train Acc: 62.71%\n",
            "\t Val. Acc: 53.51%\n",
            "\tTrain Loss: -146658.782 | Train Acc: 63.21%\n",
            "\t Val. Acc: 56.93%\n",
            "\tTrain Loss: -154258.360 | Train Acc: 63.64%\n",
            "\t Val. Acc: 57.22%\n",
            "\tTrain Loss: -172214.736 | Train Acc: 63.96%\n",
            "\t Val. Acc: 50.68%\n",
            "\tTrain Loss: -180034.944 | Train Acc: 64.57%\n",
            "\t Val. Acc: 53.52%\n",
            "\tTrain Loss: -190106.334 | Train Acc: 67.40%\n",
            "\t Val. Acc: 54.65%\n",
            "\tTrain Loss: -195225.561 | Train Acc: 65.01%\n",
            "\t Val. Acc: 56.07%\n",
            "\tTrain Loss: -203879.210 | Train Acc: 63.36%\n",
            "\t Val. Acc: 55.22%\n",
            "\tTrain Loss: -225924.290 | Train Acc: 67.40%\n",
            "\t Val. Acc: 56.21%\n",
            "\tTrain Loss: -239452.022 | Train Acc: 68.19%\n",
            "\t Val. Acc: 53.79%\n",
            "time:28.703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm6tfXInHkK_",
        "outputId": "d710856e-f0c4-414b-902c-d6c7dfb11ef9"
      },
      "source": [
        "evaluate(modelsenti, test_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4789514487439936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "8rhWO46g2qEP",
        "outputId": "c90a5f1c-9995-4f5d-c4c5-313ddef3f174"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(val_acc,label=\"val\")\n",
        "plt.plot(acc,label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Accuarcy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkFkrOY13c8o"
      },
      "source": [
        "## Evaluate test data on sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTbLWO5Pf15G"
      },
      "source": [
        "test = open('test.csv', \"r+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a6vVYb-ghOF",
        "outputId": "ff2ac841-577d-40d5-cd77-e500f42e6360"
      },
      "source": [
        "file1 = open(\"myfile.txt\",\"w+\")\n",
        "\n",
        "for i in test:\n",
        "  text = i.split(',')[0]\n",
        "  indexed_sentence = [TEXT.vocab.stoi[t] for t in text]\n",
        "  tensor = torch.LongTensor(indexed_sentence).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "\n",
        "  text_lengths = torch.as_tensor(torch.as_tensor([len(text)]), dtype=torch.int64, device='cpu')\n",
        "  predictions = torch.sigmoid(modelsenti(tensor, text_lengths))\n",
        "  pred = predictions.item()\n",
        "  i = i[:-1]\n",
        "  if pred > 0.5:\n",
        "    i += \",positive\\n\"\n",
        "  else:\n",
        "    i += \",negative\\n\"\n",
        "\n",
        "  file1.write(i)\n",
        "\n",
        "file1.seek(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMyJanmL448L"
      },
      "source": [
        "# **Train a stance detection model and test our dataset on it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpiOjgIw4JaG"
      },
      "source": [
        "## Import data and train the stance model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJRHhpQvheLd"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTbPrZE_f4_-"
      },
      "source": [
        "train = open('traindata.txt', \"r+\")\n",
        "test = open('testdata.txt', \"r+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aFDb2Koj2vD"
      },
      "source": [
        "def create_df(trn):\n",
        "  m_id = [1, 2, 3, 4, 7, 8, 9, 10]\n",
        "  val_df = []\n",
        "  trn_df = []\n",
        "  df_lst = []\n",
        "\n",
        "  for j in m_id:\n",
        "    text_lst = []\n",
        "    mid_lst = []\n",
        "    senti_lst = []\n",
        "    stance_lst = []\n",
        "    dic = {}\n",
        "\n",
        "    for i, val in enumerate(trn):\n",
        "      if i != 0:\n",
        "        splitted = val.split(',')\n",
        "        if j == int(splitted[1]):\n",
        "          senti = splitted[-1]\n",
        "          if senti == \"positive\":\n",
        "            senti = 1\n",
        "          else:\n",
        "            senti = 0\n",
        "          text_lst.append(splitted[0])\n",
        "          mid_lst.append(splitted[1])\n",
        "          senti_lst.append(senti)\n",
        "          stance_lst.append(splitted[2])\n",
        "\n",
        "    dic[\"text\"] = text_lst\n",
        "    dic[\"m_id\"] = mid_lst\n",
        "    dic[\"sentiment\"] = senti_lst\n",
        "    dic[\"stance\"] = stance_lst\n",
        "    df = pd.DataFrame(data=dic)\n",
        "\n",
        "    df_lst.append(df)\n",
        "    trn.seek(0)\n",
        "\n",
        "  return df_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7WdPMSKgDLV"
      },
      "source": [
        "traindf= create_df(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfBUfkQ-UyDK"
      },
      "source": [
        "testdf = create_df(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYoZcj9Y0ytJ",
        "outputId": "538cd450-85cf-4767-8e34-054946b8ce89"
      },
      "source": [
        "traindf[4].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>m_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>covid vaccin alter person dna</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>not_relevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>covid  vaccin turn peopl catgirl</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>not_relevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cdf rule covid vaccin moral accept receiv covi...</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>no_stance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>covid vaccin chemtrail</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>not_relevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pro lifer refrain url qt next question governo...</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...        stance\n",
              "0                     covid vaccin alter person dna   ...  not_relevant\n",
              "1                  covid  vaccin turn peopl catgirl   ...  not_relevant\n",
              "2  cdf rule covid vaccin moral accept receiv covi...  ...     no_stance\n",
              "3                            covid vaccin chemtrail   ...  not_relevant\n",
              "4  pro lifer refrain url qt next question governo...  ...         agree\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0GBYZFPnV3b"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arbu71t7XvI8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from keras.utils import to_categorical\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "Xts, X2ts, yts = [], [], []\n",
        "\n",
        "def generate_model(traindf, valdf):\n",
        "  global X_train, y_train, X_tst, y_tst, X2_train, X2_tst\n",
        "  global Xts, X2ts, yts\n",
        "  X_train = []\n",
        "  sentences = list(traindf[\"text\"])\n",
        "  for sen in sentences:\n",
        "      X_train.append(preprocess_text(\" \".join(sen)))\n",
        "\n",
        "  y_train = traindf['stance']\n",
        "\n",
        "  X_tst = []\n",
        "  sentences = list(valdf[\"text\"])\n",
        "  for sen in sentences:\n",
        "      X_tst.append(preprocess_text(\" \".join(sen)))\n",
        "\n",
        "  y_tst = valdf['stance']\n",
        "  \n",
        "\n",
        "  # label_encoder object knows how to understand word labels.\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "  # Encode labels in column 'species'.\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_tst = label_encoder.fit_transform(y_tst)\n",
        "\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_tst = to_categorical(y_tst)\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=5000)\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "  X_train = tokenizer.texts_to_sequences(X_train)\n",
        "  X_tst = tokenizer.texts_to_sequences(X_tst)\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  maxlen = 200\n",
        "\n",
        "  X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "  X_tst = pad_sequences(X_tst, padding='post', maxlen=maxlen)\n",
        "\n",
        "  Xts.append(X_tst)\n",
        "  yts.append(y_tst)\n",
        "\n",
        "  embeddings_dictionary = dict()\n",
        "  glove_file = open(\"glove.6B.100d.txt\", 'r')\n",
        "\n",
        "  for line in glove_file:\n",
        "      records = line.split()\n",
        "      word = records[0]\n",
        "      vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "      embeddings_dictionary [word] = vector_dimensions\n",
        "\n",
        "  glove_file.close()\n",
        "\n",
        "  embedding_matrix = zeros((vocab_size, 100))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_dictionary.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[index] = embedding_vector\n",
        "\n",
        "  X2_train = traindf[\"sentiment\"].values\n",
        "  X2_tst = valdf[\"sentiment\"].values\n",
        "\n",
        "  X2ts.append(X2_tst)\n",
        "\n",
        "  maxlen = 200\n",
        "  input_1 = Input(shape=(maxlen,))\n",
        "\n",
        "  input_2 = Input(shape=(1,))\n",
        "\n",
        "  embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\n",
        "  LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
        "\n",
        "  dense_layer_1 = Dense(10, activation='relu')(input_2)\n",
        "  dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
        "\n",
        "  concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n",
        "  dense_layer_3 = Dense(10, activation='relu')(concat_layer)\n",
        "  output = Dense(4, activation='softmax')(dense_layer_3)\n",
        "  model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fyeA2AdZApl",
        "outputId": "a85c9151-3538-4f59-b9aa-eb56a9ba2f4e"
      },
      "source": [
        "model_lst = []\n",
        "history_lst = []\n",
        "\n",
        "# from keras.utils import plot_model\n",
        "\n",
        "\n",
        "for i in range(len(traindf)):\n",
        "  model = generate_model(traindf[i], testdf[i])\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "  # plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)\n",
        "  # break\n",
        "  history = model.fit(x=[X_train, X2_train], y=y_train, batch_size=128, epochs=30, verbose=1, validation_split=0.2)\n",
        "  model_lst.append(model)\n",
        "  history_lst.append(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2/2 [==============================] - 3s 481ms/step - loss: 1.3856 - acc: 0.3079 - val_loss: 1.3742 - val_acc: 0.3556\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 1.3758 - acc: 0.3183 - val_loss: 1.3555 - val_acc: 0.3556\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3610 - acc: 0.3079 - val_loss: 1.3236 - val_acc: 0.3556\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3344 - acc: 0.3131 - val_loss: 1.2359 - val_acc: 0.3556\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.2708 - acc: 0.3131 - val_loss: 1.1139 - val_acc: 0.3556\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2031 - acc: 0.3079 - val_loss: 1.0928 - val_acc: 0.3556\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1833 - acc: 0.3079 - val_loss: 1.0784 - val_acc: 0.3556\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.1954 - acc: 0.3808 - val_loss: 1.0302 - val_acc: 0.5333\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 1.1717 - acc: 0.4933 - val_loss: 0.9897 - val_acc: 0.5333\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.2125 - acc: 0.4959 - val_loss: 0.9892 - val_acc: 0.5333\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1633 - acc: 0.5063 - val_loss: 1.0056 - val_acc: 0.5333\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.1435 - acc: 0.5089 - val_loss: 1.0495 - val_acc: 0.5333\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.1557 - acc: 0.4985 - val_loss: 1.0506 - val_acc: 0.5333\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.1458 - acc: 0.5126 - val_loss: 1.0026 - val_acc: 0.5333\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.1157 - acc: 0.5174 - val_loss: 1.3561 - val_acc: 0.3556\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3473 - acc: 0.3916 - val_loss: 0.9678 - val_acc: 0.5333\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.1301 - acc: 0.5048 - val_loss: 0.9987 - val_acc: 0.5333\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1505 - acc: 0.4959 - val_loss: 1.0900 - val_acc: 0.5333\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1691 - acc: 0.5011 - val_loss: 1.1236 - val_acc: 0.5333\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1894 - acc: 0.5037 - val_loss: 1.0817 - val_acc: 0.5333\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.1582 - acc: 0.5011 - val_loss: 0.9812 - val_acc: 0.5333\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1701 - acc: 0.5167 - val_loss: 0.9660 - val_acc: 0.5333\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1652 - acc: 0.5063 - val_loss: 1.1043 - val_acc: 0.5333\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1915 - acc: 0.4933 - val_loss: 1.1310 - val_acc: 0.5333\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1950 - acc: 0.5115 - val_loss: 1.1087 - val_acc: 0.5333\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1790 - acc: 0.5037 - val_loss: 1.0147 - val_acc: 0.5333\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1251 - acc: 0.5167 - val_loss: 0.9754 - val_acc: 0.5333\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1502 - acc: 0.5063 - val_loss: 0.9744 - val_acc: 0.5333\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.1045 - acc: 0.5063 - val_loss: 1.0455 - val_acc: 0.5333\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1470 - acc: 0.4933 - val_loss: 1.0642 - val_acc: 0.5333\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 467ms/step - loss: 1.3862 - acc: 0.2422 - val_loss: 1.3811 - val_acc: 0.5143\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3815 - acc: 0.3843 - val_loss: 1.3731 - val_acc: 0.5143\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3749 - acc: 0.3869 - val_loss: 1.3624 - val_acc: 0.5143\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3657 - acc: 0.3869 - val_loss: 1.3455 - val_acc: 0.5143\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3512 - acc: 0.3973 - val_loss: 1.3211 - val_acc: 0.5143\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 1.3481 - acc: 0.3947 - val_loss: 1.3205 - val_acc: 0.5143\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3374 - acc: 0.3999 - val_loss: 1.3244 - val_acc: 0.5143\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3326 - acc: 0.3973 - val_loss: 1.3342 - val_acc: 0.5143\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3427 - acc: 0.3947 - val_loss: 1.3403 - val_acc: 0.5143\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3512 - acc: 0.3843 - val_loss: 1.3436 - val_acc: 0.5143\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3531 - acc: 0.3921 - val_loss: 1.3448 - val_acc: 0.5143\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3527 - acc: 0.3999 - val_loss: 1.3451 - val_acc: 0.5143\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.3559 - acc: 0.3895 - val_loss: 1.3447 - val_acc: 0.5143\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.3558 - acc: 0.3921 - val_loss: 1.3435 - val_acc: 0.5143\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3549 - acc: 0.3895 - val_loss: 1.3418 - val_acc: 0.5143\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3533 - acc: 0.3973 - val_loss: 1.3398 - val_acc: 0.5143\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3520 - acc: 0.3947 - val_loss: 1.3378 - val_acc: 0.5143\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3526 - acc: 0.3869 - val_loss: 1.3353 - val_acc: 0.5143\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.3504 - acc: 0.3947 - val_loss: 1.3324 - val_acc: 0.5143\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 1.3469 - acc: 0.3921 - val_loss: 1.3292 - val_acc: 0.5143\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3460 - acc: 0.3869 - val_loss: 1.3257 - val_acc: 0.5143\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3412 - acc: 0.3973 - val_loss: 1.3220 - val_acc: 0.5143\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.3402 - acc: 0.3921 - val_loss: 1.3184 - val_acc: 0.5143\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.3372 - acc: 0.3947 - val_loss: 1.3144 - val_acc: 0.5143\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.3336 - acc: 0.3947 - val_loss: 1.3107 - val_acc: 0.5143\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.3317 - acc: 0.3973 - val_loss: 1.3075 - val_acc: 0.5143\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3316 - acc: 0.3947 - val_loss: 1.3042 - val_acc: 0.5143\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3300 - acc: 0.3947 - val_loss: 1.3000 - val_acc: 0.5143\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3296 - acc: 0.3921 - val_loss: 1.2946 - val_acc: 0.5143\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3256 - acc: 0.3973 - val_loss: 1.2880 - val_acc: 0.5143\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 469ms/step - loss: 1.3860 - acc: 0.2769 - val_loss: 1.3749 - val_acc: 0.6875\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 1.3801 - acc: 0.5026 - val_loss: 1.3581 - val_acc: 0.6875\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 1.3713 - acc: 0.4948 - val_loss: 1.3270 - val_acc: 0.6875\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 1.3551 - acc: 0.5000 - val_loss: 1.2518 - val_acc: 0.6875\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3190 - acc: 0.4922 - val_loss: 1.0330 - val_acc: 0.6875\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 1.3724 - acc: 0.4948 - val_loss: 1.1377 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3050 - acc: 0.4870 - val_loss: 1.2027 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3006 - acc: 0.4948 - val_loss: 1.2142 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.2955 - acc: 0.5078 - val_loss: 1.2025 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2984 - acc: 0.4818 - val_loss: 1.1646 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2735 - acc: 0.4896 - val_loss: 1.0739 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2381 - acc: 0.4974 - val_loss: 0.9751 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2260 - acc: 0.5000 - val_loss: 1.3480 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3803 - acc: 0.5130 - val_loss: 1.3219 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3739 - acc: 0.5000 - val_loss: 1.3019 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3553 - acc: 0.4922 - val_loss: 1.2499 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3231 - acc: 0.5026 - val_loss: 1.1739 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2966 - acc: 0.4922 - val_loss: 1.1264 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2634 - acc: 0.5130 - val_loss: 1.1004 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2648 - acc: 0.4948 - val_loss: 1.0797 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2669 - acc: 0.4844 - val_loss: 1.0673 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2351 - acc: 0.5130 - val_loss: 1.0502 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.2329 - acc: 0.5130 - val_loss: 1.0369 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2301 - acc: 0.5078 - val_loss: 1.0287 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2498 - acc: 0.5000 - val_loss: 1.0200 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2336 - acc: 0.5026 - val_loss: 1.0218 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.2425 - acc: 0.5000 - val_loss: 1.0227 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.2451 - acc: 0.4922 - val_loss: 1.0276 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2329 - acc: 0.5000 - val_loss: 1.0273 - val_acc: 0.6875\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.2285 - acc: 0.5026 - val_loss: 1.0232 - val_acc: 0.6875\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 463ms/step - loss: 1.3863 - acc: 0.5562 - val_loss: 1.3856 - val_acc: 0.3529\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3823 - acc: 0.5562 - val_loss: 1.3839 - val_acc: 0.3529\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 1.3750 - acc: 0.5484 - val_loss: 1.3805 - val_acc: 0.3529\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3627 - acc: 0.5562 - val_loss: 1.3751 - val_acc: 0.3529\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3449 - acc: 0.5536 - val_loss: 1.3670 - val_acc: 0.3529\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3116 - acc: 0.5536 - val_loss: 1.3806 - val_acc: 0.3529\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2303 - acc: 0.5562 - val_loss: 1.5007 - val_acc: 0.3529\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2060 - acc: 0.5536 - val_loss: 1.3686 - val_acc: 0.3529\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2437 - acc: 0.5484 - val_loss: 1.3634 - val_acc: 0.3529\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 1.2579 - acc: 0.5536 - val_loss: 1.3632 - val_acc: 0.3529\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2594 - acc: 0.5536 - val_loss: 1.3637 - val_acc: 0.3529\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2544 - acc: 0.5562 - val_loss: 1.3647 - val_acc: 0.3529\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2536 - acc: 0.5484 - val_loss: 1.3669 - val_acc: 0.3529\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2434 - acc: 0.5510 - val_loss: 1.3728 - val_acc: 0.3529\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2285 - acc: 0.5536 - val_loss: 1.3850 - val_acc: 0.3529\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2172 - acc: 0.5484 - val_loss: 1.4155 - val_acc: 0.3529\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1963 - acc: 0.5562 - val_loss: 1.4493 - val_acc: 0.3529\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1970 - acc: 0.5536 - val_loss: 1.4528 - val_acc: 0.3529\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1984 - acc: 0.5510 - val_loss: 1.4675 - val_acc: 0.3529\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1976 - acc: 0.5510 - val_loss: 1.5102 - val_acc: 0.3529\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.1984 - acc: 0.5536 - val_loss: 1.5393 - val_acc: 0.3529\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1991 - acc: 0.5562 - val_loss: 1.4884 - val_acc: 0.3529\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.1960 - acc: 0.5510 - val_loss: 1.4376 - val_acc: 0.3529\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 1.1891 - acc: 0.5536 - val_loss: 1.4158 - val_acc: 0.3529\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1886 - acc: 0.5562 - val_loss: 1.4012 - val_acc: 0.3529\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1970 - acc: 0.5536 - val_loss: 1.3918 - val_acc: 0.3529\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1985 - acc: 0.5562 - val_loss: 1.3854 - val_acc: 0.3529\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.2066 - acc: 0.5536 - val_loss: 1.3806 - val_acc: 0.3529\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.2105 - acc: 0.5562 - val_loss: 1.3776 - val_acc: 0.3529\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.2164 - acc: 0.5536 - val_loss: 1.3759 - val_acc: 0.3529\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 468ms/step - loss: 1.3862 - acc: 0.5023 - val_loss: 1.3837 - val_acc: 0.2703\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3823 - acc: 0.3235 - val_loss: 1.3786 - val_acc: 0.4595\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3754 - acc: 0.5147 - val_loss: 1.3708 - val_acc: 0.4595\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3647 - acc: 0.5173 - val_loss: 1.3572 - val_acc: 0.4595\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3460 - acc: 0.5121 - val_loss: 1.3227 - val_acc: 0.4595\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.2943 - acc: 0.5121 - val_loss: 1.2518 - val_acc: 0.4595\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.1314 - acc: 0.5095 - val_loss: 1.4035 - val_acc: 0.4595\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1751 - acc: 0.5147 - val_loss: 1.3616 - val_acc: 0.4595\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1577 - acc: 0.5095 - val_loss: 1.2950 - val_acc: 0.4595\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.1278 - acc: 0.5199 - val_loss: 1.2635 - val_acc: 0.4595\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.1365 - acc: 0.5095 - val_loss: 1.2575 - val_acc: 0.4595\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.1376 - acc: 0.5173 - val_loss: 1.2552 - val_acc: 0.4595\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1598 - acc: 0.5043 - val_loss: 1.2514 - val_acc: 0.4595\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1564 - acc: 0.5147 - val_loss: 1.2478 - val_acc: 0.4595\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.1497 - acc: 0.5069 - val_loss: 1.2490 - val_acc: 0.4595\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.1288 - acc: 0.5147 - val_loss: 1.2549 - val_acc: 0.4595\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.1200 - acc: 0.5173 - val_loss: 1.2603 - val_acc: 0.4595\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1202 - acc: 0.5173 - val_loss: 1.2626 - val_acc: 0.4595\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1250 - acc: 0.5147 - val_loss: 1.2631 - val_acc: 0.4595\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1398 - acc: 0.5043 - val_loss: 1.2656 - val_acc: 0.4595\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.1253 - acc: 0.5147 - val_loss: 1.2703 - val_acc: 0.4595\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1203 - acc: 0.5173 - val_loss: 1.2727 - val_acc: 0.4595\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1277 - acc: 0.5043 - val_loss: 1.2717 - val_acc: 0.4595\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1254 - acc: 0.5147 - val_loss: 1.2688 - val_acc: 0.4595\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1369 - acc: 0.5069 - val_loss: 1.2674 - val_acc: 0.4595\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1289 - acc: 0.5121 - val_loss: 1.2682 - val_acc: 0.4595\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1324 - acc: 0.5017 - val_loss: 1.2692 - val_acc: 0.4595\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.1362 - acc: 0.5069 - val_loss: 1.2713 - val_acc: 0.4595\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.1290 - acc: 0.5069 - val_loss: 1.2751 - val_acc: 0.4595\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 1.1290 - acc: 0.5095 - val_loss: 1.2776 - val_acc: 0.4595\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 477ms/step - loss: 1.3857 - acc: 0.4346 - val_loss: 1.3811 - val_acc: 0.2750\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3738 - acc: 0.4457 - val_loss: 1.3737 - val_acc: 0.2750\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3550 - acc: 0.4509 - val_loss: 1.3612 - val_acc: 0.2750\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3149 - acc: 0.4405 - val_loss: 1.3899 - val_acc: 0.2750\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 1.2892 - acc: 0.4483 - val_loss: 1.5349 - val_acc: 0.2750\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2791 - acc: 0.4457 - val_loss: 1.3549 - val_acc: 0.2750\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2707 - acc: 0.4379 - val_loss: 1.3525 - val_acc: 0.2750\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.2869 - acc: 0.4483 - val_loss: 1.3526 - val_acc: 0.2750\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.2965 - acc: 0.4379 - val_loss: 1.3520 - val_acc: 0.2750\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2892 - acc: 0.4587 - val_loss: 1.3509 - val_acc: 0.2750\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.2928 - acc: 0.4379 - val_loss: 1.3497 - val_acc: 0.2750\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 1.2866 - acc: 0.4431 - val_loss: 1.3484 - val_acc: 0.2750\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.2811 - acc: 0.4457 - val_loss: 1.3473 - val_acc: 0.2750\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.2746 - acc: 0.4483 - val_loss: 1.3465 - val_acc: 0.2750\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2706 - acc: 0.4274 - val_loss: 1.3464 - val_acc: 0.2750\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2598 - acc: 0.4379 - val_loss: 1.3475 - val_acc: 0.2750\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2495 - acc: 0.4457 - val_loss: 1.3502 - val_acc: 0.2750\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2414 - acc: 0.4457 - val_loss: 1.3551 - val_acc: 0.2750\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.2297 - acc: 0.4509 - val_loss: 1.3627 - val_acc: 0.2750\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2267 - acc: 0.4483 - val_loss: 1.3733 - val_acc: 0.2750\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.2214 - acc: 0.4457 - val_loss: 1.3872 - val_acc: 0.2750\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2210 - acc: 0.4405 - val_loss: 1.4033 - val_acc: 0.2750\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2146 - acc: 0.4561 - val_loss: 1.4166 - val_acc: 0.2750\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2221 - acc: 0.4535 - val_loss: 1.4212 - val_acc: 0.2750\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2134 - acc: 0.4535 - val_loss: 1.4167 - val_acc: 0.2750\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.2292 - acc: 0.4405 - val_loss: 1.4070 - val_acc: 0.2750\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2155 - acc: 0.4431 - val_loss: 1.3992 - val_acc: 0.2750\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.2106 - acc: 0.4587 - val_loss: 1.3903 - val_acc: 0.2750\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.2222 - acc: 0.4379 - val_loss: 1.3828 - val_acc: 0.2750\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.2173 - acc: 0.4457 - val_loss: 1.3795 - val_acc: 0.2750\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 460ms/step - loss: 1.3864 - acc: 0.2789 - val_loss: 1.3818 - val_acc: 0.4146\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3850 - acc: 0.3046 - val_loss: 1.3769 - val_acc: 0.4146\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3836 - acc: 0.2942 - val_loss: 1.3695 - val_acc: 0.4146\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3813 - acc: 0.3046 - val_loss: 1.3580 - val_acc: 0.4146\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3794 - acc: 0.2863 - val_loss: 1.3394 - val_acc: 0.4146\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3747 - acc: 0.2994 - val_loss: 1.3039 - val_acc: 0.4146\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3720 - acc: 0.2968 - val_loss: 1.2909 - val_acc: 0.4146\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3680 - acc: 0.2994 - val_loss: 1.3135 - val_acc: 0.4146\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3735 - acc: 0.2968 - val_loss: 1.3248 - val_acc: 0.4146\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.3732 - acc: 0.3098 - val_loss: 1.3282 - val_acc: 0.4146\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3743 - acc: 0.2994 - val_loss: 1.3283 - val_acc: 0.4146\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3739 - acc: 0.3020 - val_loss: 1.3260 - val_acc: 0.4146\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3732 - acc: 0.3072 - val_loss: 1.3226 - val_acc: 0.4146\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3704 - acc: 0.3098 - val_loss: 1.3203 - val_acc: 0.4146\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3721 - acc: 0.2968 - val_loss: 1.3199 - val_acc: 0.4146\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3731 - acc: 0.3020 - val_loss: 1.3193 - val_acc: 0.4146\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3734 - acc: 0.3020 - val_loss: 1.3175 - val_acc: 0.4146\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3727 - acc: 0.3046 - val_loss: 1.3147 - val_acc: 0.4146\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3760 - acc: 0.2968 - val_loss: 1.3103 - val_acc: 0.4146\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3723 - acc: 0.3020 - val_loss: 1.3027 - val_acc: 0.4146\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3683 - acc: 0.3098 - val_loss: 1.2993 - val_acc: 0.4146\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3738 - acc: 0.2968 - val_loss: 1.3014 - val_acc: 0.4146\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3726 - acc: 0.2994 - val_loss: 1.3016 - val_acc: 0.4146\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3693 - acc: 0.3046 - val_loss: 1.3024 - val_acc: 0.4146\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 1.3736 - acc: 0.2968 - val_loss: 1.3044 - val_acc: 0.4146\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 1.3732 - acc: 0.2942 - val_loss: 1.3035 - val_acc: 0.4146\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3740 - acc: 0.2968 - val_loss: 1.2999 - val_acc: 0.4146\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3723 - acc: 0.3072 - val_loss: 1.2940 - val_acc: 0.4146\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3737 - acc: 0.3046 - val_loss: 1.2874 - val_acc: 0.4146\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3741 - acc: 0.2863 - val_loss: 1.2792 - val_acc: 0.4146\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 2s 466ms/step - loss: 1.3863 - acc: 0.2630 - val_loss: 1.3848 - val_acc: 0.3023\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 1.3856 - acc: 0.2830 - val_loss: 1.3817 - val_acc: 0.3023\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3844 - acc: 0.2856 - val_loss: 1.3783 - val_acc: 0.3023\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3828 - acc: 0.2960 - val_loss: 1.3758 - val_acc: 0.3023\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3824 - acc: 0.2856 - val_loss: 1.3729 - val_acc: 0.3023\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3797 - acc: 0.3038 - val_loss: 1.3684 - val_acc: 0.3023\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3801 - acc: 0.2804 - val_loss: 1.3644 - val_acc: 0.3023\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3774 - acc: 0.2960 - val_loss: 1.3613 - val_acc: 0.3023\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3795 - acc: 0.2804 - val_loss: 1.3618 - val_acc: 0.3023\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3755 - acc: 0.2908 - val_loss: 1.3622 - val_acc: 0.3023\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3787 - acc: 0.2856 - val_loss: 1.3669 - val_acc: 0.3023\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3764 - acc: 0.2934 - val_loss: 1.3692 - val_acc: 0.3023\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 1.3771 - acc: 0.2856 - val_loss: 1.3674 - val_acc: 0.3023\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3706 - acc: 0.3038 - val_loss: 1.3667 - val_acc: 0.3023\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3708 - acc: 0.2986 - val_loss: 1.3671 - val_acc: 0.3023\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3680 - acc: 0.2986 - val_loss: 1.3673 - val_acc: 0.3023\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3706 - acc: 0.2934 - val_loss: 1.3669 - val_acc: 0.3023\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.3679 - acc: 0.2934 - val_loss: 1.3654 - val_acc: 0.3023\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.3649 - acc: 0.2986 - val_loss: 1.3645 - val_acc: 0.3023\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3658 - acc: 0.3013 - val_loss: 1.3653 - val_acc: 0.3488\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3661 - acc: 0.3026 - val_loss: 1.3674 - val_acc: 0.3488\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.3719 - acc: 0.2948 - val_loss: 1.3720 - val_acc: 0.3488\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3684 - acc: 0.2896 - val_loss: 1.3781 - val_acc: 0.3488\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3680 - acc: 0.2896 - val_loss: 1.3751 - val_acc: 0.3488\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 1.3667 - acc: 0.2974 - val_loss: 1.3722 - val_acc: 0.3488\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3644 - acc: 0.3052 - val_loss: 1.3709 - val_acc: 0.3488\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 1.3627 - acc: 0.2922 - val_loss: 1.3702 - val_acc: 0.3488\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.3659 - acc: 0.2974 - val_loss: 1.3693 - val_acc: 0.3488\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.3636 - acc: 0.3000 - val_loss: 1.3685 - val_acc: 0.3488\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 1.3668 - acc: 0.2974 - val_loss: 1.3681 - val_acc: 0.3488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3FmeMgOE3Cz"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for i in range(len(history_lst)):\n",
        "#   plt.plot(history_lst[i].history['acc'])\n",
        "#   plt.plot(history_lst[i].history['val_acc'])\n",
        "\n",
        "#   plt.title('model accuracy')\n",
        "#   plt.ylabel('accuracy')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train','test'], loc='upper left')\n",
        "#   plt.show()\n",
        "\n",
        "#   plt.plot(history_lst[i].history['loss'])\n",
        "#   plt.plot(history_lst[i].history['val_loss'])\n",
        "\n",
        "#   plt.title('model loss')\n",
        "#   plt.ylabel('loss')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train','test'], loc='upper left')\n",
        "#   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWaxJn4wTlkx"
      },
      "source": [
        "## Finding stance for multiple tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR0KmL5qEQG9"
      },
      "source": [
        "Xts = []\n",
        "X2ts = []\n",
        "\n",
        "def generate_partitioned_data(valdf):\n",
        "  global X_tst, X2_tst\n",
        "  global Xts, X2ts\n",
        "\n",
        "  X_tst = []\n",
        "  sentences = list(valdf[\"text\"])\n",
        "  for sen in sentences:\n",
        "      X_tst.append(preprocess_text(\" \".join(sen)))\n",
        "  \n",
        "\n",
        "  # label_encoder object knows how to understand word labels.\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "  tokenizer = Tokenizer(num_words=5000)\n",
        "  tokenizer.fit_on_texts(X_tst)\n",
        "\n",
        "  X_tst = tokenizer.texts_to_sequences(X_tst)\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  maxlen = 200\n",
        "\n",
        "  X_tst = pad_sequences(X_tst, padding='post', maxlen=maxlen)\n",
        "  Xts.append(X_tst)\n",
        "\n",
        "  embeddings_dictionary = dict()\n",
        "  glove_file = open(\"glove.6B.100d.txt\", 'r')\n",
        "\n",
        "  for line in glove_file:\n",
        "      records = line.split()\n",
        "      word = records[0]\n",
        "      vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "      embeddings_dictionary [word] = vector_dimensions\n",
        "\n",
        "  glove_file.close()\n",
        "\n",
        "  embedding_matrix = zeros((vocab_size, 100))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_dictionary.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[index] = embedding_vector\n",
        "\n",
        "  X2_tst = valdf[\"sentiment\"].values\n",
        "\n",
        "  X2ts.append(X2_tst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgHphm_1MvYX"
      },
      "source": [
        "def create_df2(trn):\n",
        "  m_id = [1, 2, 3, 4, 7, 8, 9, 10]\n",
        "  df_lst = []\n",
        "\n",
        "  for j in m_id:\n",
        "    text_lst = []\n",
        "    mid_lst = []\n",
        "    senti_lst = []\n",
        "    dic = {}\n",
        "\n",
        "    for i, val in enumerate(trn):\n",
        "      if i != 0:\n",
        "        splitted = val.split(',')\n",
        "        if j == int(splitted[1]):\n",
        "          senti = splitted[-1]\n",
        "          if senti == \"positive\\n\":\n",
        "            senti = 1\n",
        "          else:\n",
        "            senti = 0\n",
        "          text_lst.append(splitted[0])\n",
        "          mid_lst.append(splitted[1])\n",
        "          senti_lst.append(senti)\n",
        "\n",
        "    dic[\"text\"] = text_lst\n",
        "    dic[\"m_id\"] = mid_lst\n",
        "    dic[\"sentiment\"] = senti_lst\n",
        "    df = pd.DataFrame(data=dic)\n",
        "\n",
        "    df_lst.append(df)\n",
        "    trn.seek(0)\n",
        "\n",
        "  return df_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecULc9JcENj8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "4c12b6ed-0d68-46bf-ae3d-f9f53de45730"
      },
      "source": [
        "test = open('myfile.txt', \"r+\")\n",
        "testdf = create_df2(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9d02350e6c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'myfile.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtestdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_df2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'myfile.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-MiDUhTMLCb"
      },
      "source": [
        "testdf[2].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn1xipqi-kT7"
      },
      "source": [
        "for i in range(len(testdf)):\n",
        "  generate_partitioned_data(testdf[i])\n",
        "\n",
        "test.seek(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1frFR2yI4UAM"
      },
      "source": [
        "text_json = io.open('test_tweets.jsonl', mode='r', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-LOPwDm4Hc8"
      },
      "source": [
        "import json\n",
        "target = [1, 2, 3, 4, 7, 8, 9, 10]\n",
        "classes = [\"agree\", \"disagree\", \"no_stance\", \"not_relevant\"]\n",
        "\n",
        "text_python = extract_json(text_json)\n",
        "\n",
        "dic = {}\n",
        "count = 0\n",
        "for line in text_python:\n",
        "    id = line.get('id')\n",
        "    text = line.get('text')\n",
        "    text = clean_tweet(text)\n",
        "    dic[text] = id\n",
        "\n",
        "data = []\n",
        "for j in range(len(testdf)):\n",
        "  txt = testdf[j]['text'].values\n",
        "  m_id = testdf[j]['m_id'].values\n",
        "  text = testdf[j]['text'].values\n",
        "  print(\"Misinformation Target:\", target[j])\n",
        "  for i in range(len(m_id)):\n",
        "    t_id = dic[txt[i]]\n",
        "    ind = target.index(int(m_id[i]))\n",
        "    print(np.array([Xts[j][i]]), np.array([X2ts[j][i]]))\n",
        "    pred = model_lst[ind].predict(x=[np.array([Xts[j][i]]), np.array([X2ts[j][i]])]).argmax(axis=-1)\n",
        "\n",
        "    data.append({\n",
        "        'tweet_id': t_id,\n",
        "        'm_id': m_id[i],\n",
        "        'stance': classes[pred[0]]\n",
        "    })\n",
        "\n",
        "with open('output.txt', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT-a4roYTqGD"
      },
      "source": [
        "## Finding stance for single tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58aiUAC0WX-5"
      },
      "source": [
        "text_json = io.open('test_tweets.jsonl', mode='r', encoding='utf-8')\n",
        "test = open('myfile.txt', \"r+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW5-zmRTTudn",
        "outputId": "f38739f1-7c60-4270-df78-dcd1f2aa6872"
      },
      "source": [
        "lines = test.read().split(\"\\n\")\n",
        "\n",
        "import json\n",
        "target = [1, 2, 3, 4, 7, 8, 9, 10]\n",
        "classes = [\"agree\", \"disagree\", \"no_stance\", \"not_relevant\"]\n",
        "\n",
        "text_python = extract_json(text_json)\n",
        "\n",
        "dic = {}\n",
        "count = 0\n",
        "for line in text_python:\n",
        "    id = line.get('id')\n",
        "    text = line.get('text')\n",
        "    text = clean_tweet(text)\n",
        "    dic[text] = id\n",
        "\n",
        "sentence, m_id, senti = lines[1].split(\",\")\n",
        "p_text = [preprocess_text(\" \".join(sentence))]\n",
        "\n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(p_text)\n",
        "\n",
        "X_tst = tokenizer.texts_to_sequences(p_text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "maxlen = 200\n",
        "\n",
        "X_tst = pad_sequences(X_tst, padding='post', maxlen=maxlen)\n",
        "\n",
        "if senti == \"negative\":\n",
        "  X2_tst = [0]\n",
        "else:\n",
        "  X2_tst = [1]\n",
        "\n",
        "pred = model_lst[target.index(int(m_id))].predict(x=[np.array(X_tst), np.array(X2_tst)]).argmax(axis=-1)\n",
        "print(\"Stance:\", classes[pred[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stance: agree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heTZbdUw4eHo"
      },
      "source": [
        "## Find model accuracy for label test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGXS19E6JvMn",
        "outputId": "d1b0ce1a-e515-428e-e195-3d153b1c4e00"
      },
      "source": [
        "target = [1, 2, 3, 4, 7, 8, 9, 10]\n",
        "tot = 0\n",
        "denom = 0\n",
        "\n",
        "for j in range(len(testdf)):\n",
        "  denom += len(testdf[j])\n",
        "  m_id = testdf[j]['m_id'].values\n",
        "  print(\"Misinformation Target:\", target[j])\n",
        "  for i in range(len(m_id)):\n",
        "    ind = target.index(int(m_id[i]))\n",
        "    score = model_lst[ind].evaluate(x=[np.array([Xts[j][i]]), np.array([X2ts[j][i]])], y=np.array([yts[j][i]]), verbose=1)\n",
        "    tot += score[1]\n",
        "\n",
        "print(\"Test Accuracy:\", tot/denom)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Misinformation Target: 1\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5308 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5308 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5308 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5103 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2280 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3096 - acc: 0.0000e+00\n",
            "Misinformation Target: 2\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4242 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.4242 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.4242 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.4697 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1724 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5156 - acc: 0.0000e+00\n",
            "Misinformation Target: 3\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.3005 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9902 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9784 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7521 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2319 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7520 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.9684 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9837 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2321 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2321 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7520 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7521 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2324 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2319 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2322 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.2324 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2324 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7520 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2319 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.2446 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.3013 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7522 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7524 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2319 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7520 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.9791 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7523 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2319 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7552 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7573 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7518 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.2925 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7521 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7518 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7518 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7522 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7522 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.2789 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7519 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.2792 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2321 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7538 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7523 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2318 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7518 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.2986 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7522 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.9615 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2319 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7521 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2321 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7519 - acc: 1.0000\n",
            "Misinformation Target: 4\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.7138 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.7138 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2139 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.2019 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8863 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.7138 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.8863 - acc: 1.0000\n",
            "Misinformation Target: 7\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.5572 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.1453 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.1453 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5572 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.5572 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.5572 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.5572 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.5572 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.2160 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6752 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6752 - acc: 1.0000\n",
            "Misinformation Target: 8\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.9376 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6970 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.6240 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8278 - acc: 0.0000e+00\n",
            "Misinformation Target: 9\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3113 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.6372 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3108 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2438 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3961 - acc: 0.0000e+00\n",
            "Misinformation Target: 10\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2634 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6726 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.6726 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6726 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2443 - acc: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.4207 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2638 - acc: 0.0000e+00\n",
            "Test Accuracy: 0.4025\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}